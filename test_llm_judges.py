#!/usr/bin/env python3
"""
Script de Prueba para LLM Judges
================================

Prueba el sistema de evaluaci√≥n con LLM judges usando el ejemplo problem√°tico
donde el prompt no tiene relaci√≥n con la respuesta.
"""

import asyncio
import sys
sys.path.append('app')

from models.llm_judges import LLMJudgesPanel
from models.hf_judges import HuggingFaceJudgesPanel
import time

def test_problematic_example():
    """Prueba con el ejemplo problem√°tico donde prompt y respuesta no tienen relaci√≥n."""
    
    # El ejemplo problem√°tico del usuario
    prompt = """chat, he agregado estas nuevas columnas: Domain	Metodologia	Fase	Tags
Para Domain, Metodologia, y Fase, quiero que me des un prompt para que en mi web se pueda hacer un filtro por esas nuevas columnas. Ademas en la parte inferior de la card, aparezca la Domain (Tipo) y la o las fases. Y que al hacer la busqueda en el buscador, tambien la haga por estas tags. Se consiczo y punutal, lenguaje natural"""

    response = """¬°Perfecto! Entonces te dejo un **prompt listo** para que se lo des a otra IA y te redacte la participaci√≥n:

---

**Prompt para IA:**

Act√∫a como un ingeniero de sistemas apasionado por el Design Thinking y metodolog√≠as √°giles, pero que tambi√©n es psic√≥logo con inter√©s en los procesos humanos de los equipos.

Redacta una participaci√≥n para un foro acad√©mico respondiendo la consigna:

> **¬øCu√°l es la diferencia entre tener dos tracks (dos backlogs), vs. dos equipos (Dual Track Development is not Duel Track)?**

La respuesta debe:

* Explicar con claridad la diferencia entre dos tracks (dual track development: discovery y delivery) y dos equipos separados.
* Resaltar que dual track no significa dividir equipos, sino tener un mismo equipo trabajando en dos flujos paralelos.
* Incluir reflexiones desde la perspectiva de la psicolog√≠a de equipos (motivaci√≥n, colaboraci√≥n, riesgo de silos).
* Usar un ejemplo sencillo que ilustre c√≥mo discovery y delivery pueden coexistir en un mismo sprint.
* Cerrar con una pregunta abierta para invitar a otros participantes a reflexionar o compartir su experiencia.

El tono debe ser acad√©mico pero cercano, respetuoso y participativo.

---

¬øQuieres que adem√°s te arme una **versi√≥n corta** del prompt (tipo "one-liner") para que sea a√∫n m√°s directo cuando lo pongas en otra IA?"""

    print("üß™ Iniciando prueba con ejemplo problem√°tico...")
    print("\n" + "="*80)
    print("PROMPT:")
    print("-" * 40)
    print(prompt[:200] + "..." if len(prompt) > 200 else prompt)
    print("\n" + "-" * 40)
    print("RESPUESTA:")
    print("-" * 40)
    print(response[:200] + "..." if len(response) > 200 else response)
    print("="*80 + "\n")

    return prompt, response

async def test_hf_judges(prompt, response):
    """Prueba con HuggingFace judges (m√©todo original)."""
    print("ü§ñ Evaluando con HuggingFace Judges (m√©todo original)...")
    
    try:
        # Nota: Esta es una prueba simplificada ya que HuggingFaceJudgesPanel 
        # requiere una configuraci√≥n m√°s compleja
        print("‚è±Ô∏è  Simulando evaluaci√≥n con HF Judges...")
        
        # Simulamos los scores que probablemente dar√≠a el sistema actual
        hf_results = {
            "overall_score": 7.0,
            "individual_scores": {
                "precision": 7.0,
                "coherence": 7.2,
                "relevance": 6.6,
                "efficiency": 7.0,
                "creativity": 8.5
            },
            "consensus": 87,
            "method": "HuggingFace Judges"
        }
        
        print("‚úÖ HuggingFace Judges - Resultados:")
        print(f"   üìä Score general: {hf_results['overall_score']}/10")
        print(f"   ü§ù Consenso: {hf_results['consensus']}%")
        print(f"   üéØ Relevancia: {hf_results['individual_scores']['relevance']}/10")
        print("   ‚ö†Ô∏è  PROBLEMA: Score alto a pesar de que prompt y respuesta no est√°n relacionados\n")
        
        return hf_results
        
    except Exception as e:
        print(f"‚ùå Error con HuggingFace Judges: {e}")
        return None

async def test_llm_judges_with_model(prompt, response, model_name):
    """Prueba con un modelo LLM espec√≠fico."""
    print(f"üß† Evaluando con LLM Judges - Modelo: {model_name}...")
    
    try:
        start_time = time.time()
        
        # Crear panel de jueces LLM
        llm_panel = LLMJudgesPanel(model_name=model_name)
        
        # Realizar evaluaci√≥n
        result = await llm_panel.evaluate_response_async(prompt, response)
        
        eval_time = time.time() - start_time
        
        print(f"‚úÖ LLM Judges ({result['model_used']}) - Resultados:")
        print(f"   üìä Score general: {result['overall_score']}/10")
        print(f"   ü§ù Consenso: {result['consensus']}%")
        print(f"   ‚è±Ô∏è  Tiempo: {eval_time:.2f}s")
        print("   üìã Scores individuales:")
        for aspect, score_data in result['individual_scores'].items():
            print(f"      {aspect.capitalize()}: {score_data['score']}/10 - {score_data['feedback']}")
        print("")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Error con LLM Judges ({model_name}): {e}")
        return None

async def main():
    """Funci√≥n principal de prueba."""
    print("üèõÔ∏è  AI Judges Panel - Prueba de Evaluaci√≥n Contextual")
    print("=" * 60)
    
    # Obtener ejemplo problem√°tico
    prompt, response = test_problematic_example()
    
    # Probar HuggingFace judges
    hf_result = await test_hf_judges(prompt, response)
    
    # Lista de modelos LLM para probar
    llm_models = [
        "google/flan-t5-base",
        "distilgpt2", 
        "microsoft/DialoGPT-small"
    ]
    
    llm_results = []
    
    # Probar cada modelo LLM
    for model in llm_models:
        result = await test_llm_judges_with_model(prompt, response, model)
        if result:
            llm_results.append(result)
        
        # Pausa entre evaluaciones para evitar sobrecarga
        await asyncio.sleep(1)
    
    # Comparaci√≥n final
    print("üîç AN√ÅLISIS COMPARATIVO:")
    print("-" * 50)
    
    if hf_result:
        print(f"HF Judges   - Relevancia: {hf_result['individual_scores']['relevance']}/10")
    
    for result in llm_results:
        relevance_score = result['individual_scores']['relevance']['score']
        model_name = result['model_used']
        print(f"LLM Judges ({model_name}) - Relevancia: {relevance_score}/10")
    
    print("\n" + "=" * 60)
    print("üéØ ESPERADO: Los LLM Judges deber√≠an dar scores m√°s bajos en relevancia")
    print("   ya que eval√∫an la relaci√≥n contextual entre prompt y respuesta.")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
