from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from typing import Dict, List, Optional, Tuple
import re
import logging
from dataclasses import dataclass
import asyncio
import json
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ModelConfig:
    name: str
    model_id: str
    max_length: int
    temperature: float = 0.7
    description: str = ""

class LLMJudgesPanel:
    """Panel de jueces que utiliza modelos de lenguaje reales para evaluaci√≥n contextual."""
    
    # Singleton pattern - cache global de modelos
    _model_cache = {}
    
    AVAILABLE_MODELS = {
        "microsoft/DialoGPT-medium": ModelConfig(
            name="DialoGPT Medium",
            model_id="microsoft/DialoGPT-medium",
            max_length=80,
            temperature=0.2,
            description="Modelo conversacional de Microsoft, bueno para evaluar coherencia contextual"
        ),
        "google/flan-t5-base": ModelConfig(
            name="FLAN-T5 Base",
            model_id="google/flan-t5-base",
            max_length=60,
            temperature=0.3,
            description="Modelo instruction-following de Google, excelente para evaluaci√≥n de relevancia"
        ),
        "distilgpt2": ModelConfig(
            name="DistilGPT2",
            model_id="distilgpt2",
            max_length=50,
            temperature=0.2,
            description="Modelo ligero de generaci√≥n de texto, r√°pido y eficiente (RECOMENDADO)"
        ),
        "microsoft/DialoGPT-small": ModelConfig(
            name="DialoGPT Small",
            model_id="microsoft/DialoGPT-small",
            max_length=50,
            temperature=0.2,
            description="Versi√≥n m√°s ligera de DialoGPT, ideal para evaluaciones r√°pidas"
        )
    }
    
    def __init__(self, model_name: str = "distilgpt2"):
        self.model_name = model_name
        self.model_config = self.AVAILABLE_MODELS.get(model_name)
        if not self.model_config:
            raise ValueError(f"Modelo {model_name} no disponible. Modelos disponibles: {list(self.AVAILABLE_MODELS.keys())}")
        
        self.pipeline = None
        self._load_model()
        
    def _load_model(self):
        """Carga el modelo seleccionado usando singleton pattern para cache."""
        # Verificar si el modelo ya est√° en cache
        if self.model_name in self._model_cache:
            logger.info(f"‚úÖ Usando modelo {self.model_config.name} desde cache")
            self.pipeline = self._model_cache[self.model_name]
            return
            
        try:
            logger.info(f"üîÑ Cargando modelo {self.model_config.name} por primera vez...")
            start_time = time.time()
            
            if "flan-t5" in self.model_name:
                self.pipeline = pipeline(
                    "text2text-generation",
                    model=self.model_config.model_id,
                    max_length=self.model_config.max_length,
                    temperature=self.model_config.temperature,
                    do_sample=False,  # M√°s r√°pido y determin√≠stico
                    device=-1  # Force CPU to avoid GPU conflicts
                )
            else:
                self.pipeline = pipeline(
                    "text-generation",
                    model=self.model_config.model_id,
                    max_length=self.model_config.max_length,
                    temperature=self.model_config.temperature,
                    do_sample=False,  # M√°s r√°pido y determin√≠stico
                    pad_token_id=50256,
                    device=-1  # Force CPU to avoid GPU conflicts
                )
            
            # Guardar en cache
            self._model_cache[self.model_name] = self.pipeline
            load_time = time.time() - start_time
            
            logger.info(f"‚úÖ Modelo {self.model_config.name} cargado en {load_time:.1f}s y guardado en cache")
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo {self.model_name}: {str(e)}")
            # Fallback a modelo m√°s simple
            fallback_model = "distilgpt2"
            if fallback_model not in self._model_cache:
                logger.info("üîÑ Cargando modelo fallback DistilGPT2...")
                self._model_cache[fallback_model] = pipeline(
                    "text-generation",
                    model=fallback_model,
                    max_length=50,
                    temperature=0.2,
                    do_sample=False,
                    pad_token_id=50256,
                    device=-1
                )
            
            self.model_name = fallback_model
            self.model_config = self.AVAILABLE_MODELS[fallback_model]
            self.pipeline = self._model_cache[fallback_model]
            logger.info("‚úÖ Fallback a DistilGPT2 completado")
    
    def _generate_evaluation_prompt(self, judge_name: str, prompt: str, response: str, criteria: str) -> str:
        """Genera el prompt para que el LLM eval√∫e como un juez espec√≠fico."""
        
        base_prompt = f"""Act√∫a como {judge_name}, un experto evaluador de respuestas de IA.

Tu tarea es evaluar la calidad de una respuesta en relaci√≥n a un prompt espec√≠fico.

PROMPT ORIGINAL: "{prompt}"
RESPUESTA A EVALUAR: "{response}"

CRITERIOS DE EVALUACI√ìN: {criteria}

Eval√∫a la respuesta en una escala del 1-10 considerando:
- ¬øLa respuesta aborda directamente lo solicitado en el prompt?
- ¬øEs relevante y √∫til para quien hizo la pregunta?
- ¬øCumple con los criterios espec√≠ficos?

Responde SOLO con un n√∫mero del 1 al 10, seguido de una breve justificaci√≥n (m√°ximo 20 palabras).
Formato: "Puntuaci√≥n: X/10. Justificaci√≥n: [raz√≥n breve]"""
        
        return base_prompt
    
    def _extract_score_from_response(self, llm_response: str) -> Tuple[float, str]:
        """Extrae la puntuaci√≥n y justificaci√≥n de la respuesta del LLM."""
        try:
            # Buscar patrones como "Puntuaci√≥n: 7/10" o "7/10" o "Score: 8"
            score_patterns = [
                r"Puntuaci√≥n:\s*(\d+(?:\.\d+)?)/10",
                r"Score:\s*(\d+(?:\.\d+)?)/10",
                r"(\d+(?:\.\d+)?)/10",
                r"(\d+(?:\.\d+)?)\s*/\s*10",
                r"(\d+(?:\.\d+)?)"
            ]
            
            score = 5.0  # Default fallback
            for pattern in score_patterns:
                match = re.search(pattern, llm_response, re.IGNORECASE)
                if match:
                    extracted_score = float(match.group(1))
                    if 0 <= extracted_score <= 10:
                        score = extracted_score
                        break
            
            # Extraer justificaci√≥n
            justification_match = re.search(r"Justificaci√≥n:\s*(.+?)(?:\n|$)", llm_response, re.IGNORECASE)
            justification = justification_match.group(1).strip() if justification_match else "Evaluaci√≥n autom√°tica"
            
            return score, justification
            
        except Exception as e:
            logger.error(f"Error extrayendo puntuaci√≥n: {str(e)}")
            return 5.0, "Error en evaluaci√≥n"
    
    def evaluate_precision(self, prompt: str, response: str) -> Dict:
        """Eval√∫a la precisi√≥n de la respuesta."""
        criteria = "Precisi√≥n factual, exactitud de la informaci√≥n, ausencia de errores, respuesta directa al prompt."
        eval_prompt = self._generate_evaluation_prompt("Dr. Precisi√≥n", prompt, response, criteria)
        
        try:
            if "flan-t5" in self.model_name:
                llm_response = self.pipeline(
                    eval_prompt, 
                    max_length=self.model_config.max_length,
                    num_return_sequences=1,
                    do_sample=False,
                    early_stopping=True
                )[0]['generated_text']
            else:
                results = self.pipeline(
                    eval_prompt, 
                    max_length=len(eval_prompt) + self.model_config.max_length,
                    num_return_sequences=1,
                    do_sample=False,
                    early_stopping=True,
                    pad_token_id=50256
                )
                llm_response = results[0]['generated_text']
                # Remover el prompt original de la respuesta
                llm_response = llm_response[len(eval_prompt):].strip()
            
            score, justification = self._extract_score_from_response(llm_response)
            
            return {
                "score": score,
                "feedback": f"[Precision] {justification}",
                "details": {
                    "raw_response": llm_response,
                    "model_used": self.model_config.name
                }
            }
        except Exception as e:
            logger.error(f"Error en evaluaci√≥n de precisi√≥n: {str(e)}")
            return {
                "score": 5.0,
                "feedback": "[Precision] Error en evaluaci√≥n autom√°tica",
                "details": {"error": str(e)}
            }
    
    def evaluate_coherence(self, prompt: str, response: str) -> Dict:
        """Eval√∫a la coherencia de la respuesta."""
        criteria = "Coherencia interna, fluidez narrativa, estructura l√≥gica, conexi√≥n clara entre ideas."
        eval_prompt = self._generate_evaluation_prompt("Prof. Coherencia", prompt, response, criteria)
        
        try:
            if "flan-t5" in self.model_name:
                llm_response = self.pipeline(eval_prompt, max_length=100, num_return_sequences=1)[0]['generated_text']
            else:
                llm_response = self.pipeline(eval_prompt, max_length=len(eval_prompt) + 100, num_return_sequences=1)[0]['generated_text']
                llm_response = llm_response[len(eval_prompt):].strip()
            
            score, justification = self._extract_score_from_response(llm_response)
            
            return {
                "score": score,
                "feedback": f"[Coherence] {justification}",
                "details": {
                    "raw_response": llm_response,
                    "model_used": self.model_config.name
                }
            }
        except Exception as e:
            logger.error(f"Error en evaluaci√≥n de coherencia: {str(e)}")
            return {
                "score": 5.0,
                "feedback": "[Coherence] Error en evaluaci√≥n autom√°tica",
                "details": {"error": str(e)}
            }
    
    def evaluate_relevance(self, prompt: str, response: str) -> Dict:
        """Eval√∫a la relevancia de la respuesta al prompt."""
        criteria = "Relevancia directa al prompt, pertinencia del contenido, enfoque en lo solicitado, utilidad para el usuario."
        eval_prompt = self._generate_evaluation_prompt("Lic. Relevancia", prompt, response, criteria)
        
        try:
            if "flan-t5" in self.model_name:
                llm_response = self.pipeline(eval_prompt, max_length=100, num_return_sequences=1)[0]['generated_text']
            else:
                llm_response = self.pipeline(eval_prompt, max_length=len(eval_prompt) + 100, num_return_sequences=1)[0]['generated_text']
                llm_response = llm_response[len(eval_prompt):].strip()
            
            score, justification = self._extract_score_from_response(llm_response)
            
            return {
                "score": score,
                "feedback": f"[Relevance] {justification}",
                "details": {
                    "raw_response": llm_response,
                    "model_used": self.model_config.name
                }
            }
        except Exception as e:
            logger.error(f"Error en evaluaci√≥n de relevancia: {str(e)}")
            return {
                "score": 5.0,
                "feedback": "[Relevance] Error en evaluaci√≥n autom√°tica",
                "details": {"error": str(e)}
            }
    
    def evaluate_efficiency(self, prompt: str, response: str) -> Dict:
        """Eval√∫a la eficiencia de la respuesta."""
        criteria = "Concisi√≥n, claridad, organizaci√≥n, facilidad de comprensi√≥n, ausencia de informaci√≥n redundante."
        eval_prompt = self._generate_evaluation_prompt("Ed. Eficiencia", prompt, response, criteria)
        
        try:
            if "flan-t5" in self.model_name:
                llm_response = self.pipeline(eval_prompt, max_length=100, num_return_sequences=1)[0]['generated_text']
            else:
                llm_response = self.pipeline(eval_prompt, max_length=len(eval_prompt) + 100, num_return_sequences=1)[0]['generated_text']
                llm_response = llm_response[len(eval_prompt):].strip()
            
            score, justification = self._extract_score_from_response(llm_response)
            
            return {
                "score": score,
                "feedback": f"[Efficiency] {justification}",
                "details": {
                    "raw_response": llm_response,
                    "model_used": self.model_config.name
                }
            }
        except Exception as e:
            logger.error(f"Error en evaluaci√≥n de eficiencia: {str(e)}")
            return {
                "score": 5.0,
                "feedback": "[Efficiency] Error en evaluaci√≥n autom√°tica",
                "details": {"error": str(e)}
            }
    
    def evaluate_creativity(self, prompt: str, response: str) -> Dict:
        """Eval√∫a la creatividad de la respuesta."""
        criteria = "Originalidad, innovaci√≥n en el enfoque, diversidad de ideas, uso creativo del lenguaje, soluciones ingeniosas."
        eval_prompt = self._generate_evaluation_prompt("Dra. Creatividad", prompt, response, criteria)
        
        try:
            if "flan-t5" in self.model_name:
                llm_response = self.pipeline(eval_prompt, max_length=100, num_return_sequences=1)[0]['generated_text']
            else:
                llm_response = self.pipeline(eval_prompt, max_length=len(eval_prompt) + 100, num_return_sequences=1)[0]['generated_text']
                llm_response = llm_response[len(eval_prompt):].strip()
            
            score, justification = self._extract_score_from_response(llm_response)
            
            return {
                "score": score,
                "feedback": f"[Creativity] {justification}",
                "details": {
                    "raw_response": llm_response,
                    "model_used": self.model_config.name
                }
            }
        except Exception as e:
            logger.error(f"Error en evaluaci√≥n de creatividad: {str(e)}")
            return {
                "score": 5.0,
                "feedback": "[Creativity] Error en evaluaci√≥n autom√°tica",
                "details": {"error": str(e)}
            }
    
    async def evaluate_response_async(self, prompt: str, response: str, weights: Optional[Dict[str, float]] = None) -> Dict:
        """Eval√∫a una respuesta usando todos los jueces de forma as√≠ncrona."""
        if weights is None:
            weights = {
                "precision": 0.35,
                "coherence": 0.30,
                "relevance": 0.20,
                "efficiency": 0.10,
                "creativity": 0.05
            }
        
        # Ejecutar evaluaciones
        precision_result = self.evaluate_precision(prompt, response)
        coherence_result = self.evaluate_coherence(prompt, response)
        relevance_result = self.evaluate_relevance(prompt, response)
        efficiency_result = self.evaluate_efficiency(prompt, response)
        creativity_result = self.evaluate_creativity(prompt, response)
        
        # Calcular puntuaci√≥n final
        final_score = (
            precision_result["score"] * weights["precision"] +
            coherence_result["score"] * weights["coherence"] +
            relevance_result["score"] * weights["relevance"] +
            efficiency_result["score"] * weights["efficiency"] +
            creativity_result["score"] * weights["creativity"]
        )
        
        # Calcular consenso (desviaci√≥n est√°ndar de las puntuaciones)
        scores = [
            precision_result["score"],
            coherence_result["score"],
            relevance_result["score"],
            efficiency_result["score"],
            creativity_result["score"]
        ]
        
        mean_score = sum(scores) / len(scores)
        variance = sum((score - mean_score) ** 2 for score in scores) / len(scores)
        std_dev = variance ** 0.5
        consensus = max(0, 100 - (std_dev * 10))  # Consenso como porcentaje
        
        return {
            "overall_score": round(final_score, 1),
            "consensus": round(consensus, 0),
            "model_used": self.model_config.name,
            "individual_scores": {
                "precision": precision_result,
                "coherence": coherence_result,
                "relevance": relevance_result,
                "efficiency": efficiency_result,
                "creativity": creativity_result
            },
            "weights": weights,
            "evaluation_stats": {
                "mean_score": round(mean_score, 2),
                "std_deviation": round(std_dev, 2),
                "score_range": {
                    "min": min(scores),
                    "max": max(scores)
                }
            }
        }
    
    def evaluate_response(self, prompt: str, response: str, weights: Optional[Dict[str, float]] = None) -> Dict:
        """Versi√≥n s√≠ncrona de la evaluaci√≥n."""
        import asyncio
        return asyncio.run(self.evaluate_response_async(prompt, response, weights))
    
    @classmethod
    def get_available_models(cls) -> Dict[str, ModelConfig]:
        """Retorna la lista de modelos disponibles."""
        return cls.AVAILABLE_MODELS
    
    @classmethod
    def get_model_info(cls, model_name: str) -> Optional[ModelConfig]:
        """Retorna informaci√≥n sobre un modelo espec√≠fico."""
        return cls.AVAILABLE_MODELS.get(model_name)
