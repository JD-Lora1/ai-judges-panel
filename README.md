# AI Judges Panel ðŸ›ï¸âš–ï¸
## Una Arquitectura Multi-Agente para EvaluaciÃ³n de LLMs

ðŸš€ **[AplicaciÃ³n Web en Vivo - https://ai-judges-ai.up.railway.app/](https://ai-judges-ai.up.railway.app/)**

### ðŸŽ¯ VisiÃ³n del Proyecto

Este proyecto implementa una **arquitectura de panel de jueces** donde mÃºltiples LLMs especializados evalÃºan las respuestas de otros LLMs desde diferentes perspectivas, combinando la robustez de mÃ©tricas automÃ¡ticas con el juicio inteligente de modelos de lenguaje.

## ðŸ—ï¸ Arquitectura: Panel de Jueces Especializados

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      SISTEMA DE EVALUACIÃ“N                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  LLM CANDIDATO  â”‚           â”‚ MÃ‰TRICAS AUTO   â”‚
            â”‚   (A evaluar)   â”‚           â”‚ BLEU/ROUGE/BERT â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚            PANEL DE JUECES                    â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
            â”‚  ðŸŽ¯ Juez de PRECISIÃ“N     ðŸ“Š Score: 0-10     â”‚
            â”‚  ðŸŽ¨ Juez de CREATIVIDAD   ðŸ“Š Score: 0-10     â”‚
            â”‚  ðŸ§  Juez de COHERENCIA    ðŸ“Š Score: 0-10     â”‚
            â”‚  ðŸŽª Juez de RELEVANCIA    ðŸ“Š Score: 0-10     â”‚
            â”‚  âš¡ Juez de EFICIENCIA    ðŸ“Š Score: 0-10     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ META-EVALUADOR  â”‚
                    â”‚   (Agregador)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ REPORTE FINAL   â”‚
                    â”‚ Score + Insightsâ”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ§  EspecializaciÃ³n de Jueces

### ðŸŽ¯ **Juez de PrecisiÃ³n** (`PrecisionJudge`)
- **Rol**: EvalÃºa factualidad y exactitud de la informaciÃ³n
- **Criterios**: Veracidad, ausencia de alucinaciones, citas correctas
- **Prompt especializado**: "Como experto en verificaciÃ³n de hechos..."

### ðŸŽ¨ **Juez de Creatividad** (`CreativityJudge`)
- **Rol**: EvalÃºa originalidad, innovaciÃ³n y pensamiento lateral
- **Criterios**: Novedad de ideas, perspectivas Ãºnicas, soluciones creativas
- **Prompt especializado**: "Como crÃ­tico de arte y literatura..."

### ðŸ§  **Juez de Coherencia** (`CoherenceJudge`)
- **Rol**: EvalÃºa lÃ³gica interna, estructura y fluidez
- **Criterios**: ArgumentaciÃ³n sÃ³lida, transiciones suaves, consistencia
- **Prompt especializado**: "Como filÃ³sofo especializado en lÃ³gica..."

### ðŸŽª **Juez de Relevancia** (`RelevanceJudge`)
- **Rol**: EvalÃºa si la respuesta aborda realmente la pregunta
- **Criterios**: Pertinencia, completitud de la respuesta
- **Prompt especializado**: "Como evaluador de comprensiÃ³n lectora..."

### âš¡ **Juez de Eficiencia** (`EfficiencyJudge`)
- **Rol**: EvalÃºa concisiÃ³n y claridad comunicativa
- **Criterios**: Brevedad sin pÃ©rdida de informaciÃ³n, claridad
- **Prompt especializado**: "Como editor profesional..."

## ðŸ“Š Sistema de AgregaciÃ³n

### **Meta-Evaluador** (`MetaEvaluator`)
- Combina scores de todos los jueces
- Aplica pesos personalizables segÃºn el contexto
- Genera un reporte explicativo detallado
- Identifica consensos y discrepancias entre jueces

### **MÃ©tricas HÃ­bridas Ponderadas**
```python
final_score = (
    0.35 * precision_score +    # Mayor peso: exactitud factual
    0.30 * coherence_score +    # Mayor peso: estructura lÃ³gica  
    0.20 * relevance_score +    # Peso medio: pertinencia al prompt
    0.10 * efficiency_score +   # Peso menor: claridad y concisiÃ³n
    0.05 * creativity_score     # Peso mÃ­nimo: originalidad
) + automatic_metrics_boost
```

> **Nota**: El sistema evalÃºa la **relaciÃ³n prompt-respuesta**, no solo la respuesta aisladamente. Cada juez considera tanto la pregunta original como la calidad de la respuesta en ese contexto especÃ­fico.

## ðŸš€ Casos de Uso

### 1. **EvaluaciÃ³n de Chatbots** ðŸ¤–
- **Prompt-Aware**: Compara cÃ³mo GPT, Claude, Gemini responden al **mismo prompt**
- **Contextual**: Identifica quÃ© modelo entiende mejor la **intenciÃ³n** del prompt
- **OptimizaciÃ³n**: Mejora prompts basado en feedback especÃ­fico de cada relaciÃ³n prompt-respuesta

### 2. **Content Generation Assessment** âœï¸
- **Pertinencia**: Â¿El contenido generado cumple **exactamente** con el briefing/prompt?
- **Coherencia Contextual**: Â¿La estructura responde a lo solicitado en el prompt?
- **PrecisiÃ³n**: Â¿Los hechos/datos generados son consistentes con los requerimientos?

### 3. **Educational AI Evaluation** ðŸŽ“
- **Relevancia PedagÃ³gica**: Â¿La explicaciÃ³n del AI aborda la pregunta del estudiante?
- **Coherencia DidÃ¡ctica**: Â¿La respuesta sigue un flujo lÃ³gico apropiado para el nivel?
- **PrecisiÃ³n Educativa**: Â¿La informaciÃ³n es factualmente correcta y verificable?

### 4. **Research Assistant Analysis** ðŸ”¬
- **Relevancia de SÃ­ntesis**: Â¿La sÃ­ntesis responde a la consulta de investigaciÃ³n?
- **PrecisiÃ³n AcadÃ©mica**: Verifica accuracy de citaciones **en relaciÃ³n al tema** consultado
- **Coherencia Conceptual**: Â¿Las conexiones propuestas son lÃ³gicas dado el contexto?

### 5. **EvaluaciÃ³n de Prompts Complejos** ðŸ§ 
- **Multi-step Instructions**: EvalÃºa cÃ³mo los LLMs manejan prompts con mÃºltiples pasos
- **Domain-Specific Queries**: Mide precisiÃ³n en respuestas tÃ©cnicas, legales, mÃ©dicas
- **Creative vs Factual Balance**: Detecta cuÃ¡ndo priorizar creatividad vs exactitud segÃºn el prompt

## ðŸ› ï¸ Estructura del Proyecto

```
ai-judges-panel/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ judges/
â”‚   â”‚   â”œâ”€â”€ base_judge.py          # Clase base para todos los jueces
â”‚   â”‚   â”œâ”€â”€ precision_judge.py     # Juez especializado en precisiÃ³n
â”‚   â”‚   â”œâ”€â”€ creativity_judge.py    # Juez especializado en creatividad
â”‚   â”‚   â”œâ”€â”€ coherence_judge.py     # Juez especializado en coherencia
â”‚   â”‚   â”œâ”€â”€ relevance_judge.py     # Juez especializado en relevancia
â”‚   â”‚   â””â”€â”€ efficiency_judge.py    # Juez especializado en eficiencia
â”‚   â”œâ”€â”€ evaluators/
â”‚   â”‚   â”œâ”€â”€ meta_evaluator.py      # Agregador inteligente de scores
â”‚   â”‚   â”œâ”€â”€ hybrid_metrics.py      # Combina mÃ©tricas auto + LLM
â”‚   â”‚   â””â”€â”€ report_generator.py    # Generador de reportes detallados
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ llm_interface.py       # Interfaz unificada para LLMs
â”‚   â”‚   â””â”€â”€ candidate_model.py     # Wrapper para modelos a evaluar
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ prompts.py             # Prompts especializados
â”‚       â”œâ”€â”€ scoring.py             # Sistema de puntuaciÃ³n
â”‚       â””â”€â”€ config.py              # Configuraciones del sistema
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_demo_basic.ipynb        # Demo bÃ¡sico del sistema
â”‚   â”œâ”€â”€ 02_judge_comparison.ipynb  # ComparaciÃ³n entre jueces
â”‚   â”œâ”€â”€ 03_model_benchmarking.ipynb# Benchmark de modelos
â”‚   â””â”€â”€ 04_custom_evaluation.ipynb # Evaluaciones personalizadas
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ chatbot_evaluation.py      # Ejemplo: evaluar chatbots
â”‚   â”œâ”€â”€ creative_writing.py        # Ejemplo: evaluar escritura creativa
â”‚   â””â”€â”€ code_generation.py         # Ejemplo: evaluar cÃ³digo generado
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_judges.py             # Tests para jueces individuales
â”‚   â”œâ”€â”€ test_meta_evaluator.py     # Tests para meta-evaluador
â”‚   â””â”€â”€ test_integration.py        # Tests de integraciÃ³n completa
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md            # DocumentaciÃ³n arquitectÃ³nica
â”‚   â”œâ”€â”€ judge_design.md            # DiseÃ±o de jueces especializados
â”‚   â””â”€â”€ evaluation_guide.md        # GuÃ­a de uso y evaluaciÃ³n
â”œâ”€â”€ requirements.txt               # Dependencias del proyecto
â””â”€â”€ README.md                      # Este archivo
```

## ðŸŽ¯ Ventajas de Esta Arquitectura

### **1. EvaluaciÃ³n Multi-Dimensional** ðŸ“
- Cada aspecto evaluado por un especialista
- Evita sesgos de evaluaciÃ³n monolÃ­tica
- Perspectivas complementarias

### **2. Interpretabilidad Completa** ðŸ”
- Cada juez explica su calificaciÃ³n
- Feedback especÃ­fico y accionable
- Transparencia en el proceso de evaluaciÃ³n

### **3. Escalabilidad Modular** ðŸ“ˆ
- FÃ¡cil agregar nuevos jueces especializados
- Pesos ajustables segÃºn contexto
- Adaptable a diferentes dominios

### **4. Robustez Contra Sesgos** ðŸ›¡ï¸
- MÃºltiples perspectivas reducen sesgos individuales
- Consenso emergente mÃ¡s confiable
- DetecciÃ³n automÃ¡tica de discrepancias

## ðŸš€ Quick Start

### OpciÃ³n 1: AplicaciÃ³n Web (Recomendado)

```bash
# Instalar dependencias
pip install -r requirements.txt

# Ejecutar aplicaciÃ³n web
uvicorn app.main:app --reload

# Abrir en navegador: http://localhost:8000
```

### OpciÃ³n 2: API ProgramÃ¡tica

```python
import asyncio
from app.models.hf_judges import HuggingFaceJudgesPanel

# Inicializar el panel de jueces con modelos de HuggingFace
async def main():
    panel = HuggingFaceJudgesPanel()
    await panel.initialize()
    
    # Evaluar relaciÃ³n prompt-respuesta (no solo la respuesta)
    result = await panel.evaluate(
        prompt="Explica quÃ© es la inteligencia artificial y da 3 ejemplos prÃ¡cticos",
        response="La IA es un campo de la informÃ¡tica que simula inteligencia humana. Ejemplos: 1) Asistentes virtuales como Siri, 2) Recomendaciones de Netflix, 3) Coches autÃ³nomos de Tesla.",
        domain="technical"
    )
    
    print(f"Score Final: {result.final_score:.1f}/10")
    print(f"Consenso: {result.consensus_level:.1%}")
    print("\nEvaluaciÃ³n por aspecto:")
    for aspect, score in result.individual_scores.items():
        print(f"  {aspect.title()}: {score:.1f}/10")
    print("\nFortalezas principales:")
    for strength in result.strengths[:3]:
        print(f"  âœ“ {strength}")

# Ejecutar
asyncio.run(main())
```

### OpciÃ³n 3: Notebooks de InvestigaciÃ³n

```bash
# Ejecutar notebooks originales
jupyter notebook notebooks/01_demo_basic.ipynb
```

## ðŸŽª Â¿Por QuÃ© Esta Arquitectura es Innovadora?

### **Diferenciadores Clave:**

1. **EspecializaciÃ³n Inteligente**: Cada juez tiene expertise especÃ­fico
2. **HÃ­brida**: Combina mÃ©tricas automÃ¡ticas + evaluaciÃ³n por LLM
3. **Explicable**: No solo scores, sino razones detalladas
4. **Consensual**: MÃºltiples perspectivas convergen en evaluaciÃ³n final
5. **Adaptativa**: Pesos ajustables segÃºn tipo de evaluaciÃ³n

---

### ðŸŽŠ **Â¡Listo para Evaluar el Futuro de la IA!**

*Este proyecto representa la evoluciÃ³n natural de la evaluaciÃ³n de IA: de mÃ©tricas simples a juicios inteligentes especializados.*

## ðŸš€ Despliegue en Railway

### Despliegue AutomÃ¡tico

1. **Fork el repositorio** en GitHub
2. **Conecta con Railway**: 
   - Ve a [railway.app](https://railway.app)
   - Conecta tu cuenta de GitHub
   - Selecciona este repositorio
3. **ConfiguraciÃ³n automÃ¡tica**:
   - Railway detectarÃ¡ automÃ¡ticamente el `railway.toml`
   - La aplicaciÃ³n se desplegarÃ¡ con FastAPI + Uvicorn
   - Los modelos de HuggingFace se cargarÃ¡n en el primer arranque

### Variables de Entorno (Opcionales)

```bash
ENVIRONMENT=production
PYTHONPATH=.
# HF_TOKEN=your_huggingface_token  # Si usas modelos privados
```

### Endpoints Disponibles

Desde la aplicaciÃ³n desplegada: **https://ai-judges-ai.up.railway.app/**

- `GET /` - [Interfaz web principal](https://ai-judges-ai.up.railway.app/)
- `GET /evaluate` - [PÃ¡gina de evaluaciÃ³n interactiva](https://ai-judges-ai.up.railway.app/evaluate)
- `POST /api/v1/evaluate` - API de evaluaciÃ³n bÃ¡sica
- `POST /api/v1/evaluate/detailed` - API de evaluaciÃ³n avanzada
- `POST /api/v1/evaluate/compare` - ComparaciÃ³n de respuestas
- `GET /api/v1/docs` - [DocumentaciÃ³n interactiva de la API](https://ai-judges-ai.up.railway.app/api/v1/docs)
- `GET /health` - Health check para Railway

**PrÃ³ximo paso**: Despliega en Railway o ejecuta `uvicorn app.main:app --reload`

## ðŸ¤– **CÃ³mo Funciona: Arquitectura TÃ©cnica**

### **ðŸŽ¨ Flujo de EvaluaciÃ³n Multi-Agente**

1. **Input**: Usuario ingresa `prompt` + `respuesta_del_LLM`
2. **Contexto**: Sistema crea `EvaluationContext` con ambos elementos
3. **EvaluaciÃ³n Paralela**: Los 5 jueces analizan simultÃ¡neamente en ~10-30 segundos
4. **Meta-AgregaciÃ³n**: Combina scores con pesos personalizados + anÃ¡lisis de consenso
5. **Output**: Score final + feedback detallado + visualizaciÃ³n

### **ðŸ¤– Modelos de Hugging Face Utilizados**

#### **ðŸŽ¯ Dr. PrecisiÃ³n** (Peso: 35%)
- **TÃ©cnica**: AnÃ¡lisis heurÃ­stico inteligente + NLP
- **EvalÃºa**: Exactitud factual, ausencia de alucinaciones, referencias vÃ¡lidas
- **Algoritmos**:
  - Detecta indicadores de incertidumbre (`"quizÃ¡s", "tal vez", "posiblemente"`)
  - Penaliza declaraciones absolutas (`"siempre", "nunca", "todos"`)
  - Busca nÃºmeros especÃ­ficos, fechas, y citaciones cientÃ­ficas
  - Analiza la relaciÃ³n entre afirmaciones del prompt y respuesta

#### **ðŸ§  Prof. Coherencia** (Peso: 30%)
- **Modelo HF**: `all-MiniLM-L6-v2` (SentenceTransformers)
- **TÃ©cnica**: Embeddings semÃ¡nticos + similitud coseno
- **EvalÃºa**: Flujo lÃ³gico, transiciones, consistencia interna
- **Proceso**:
  1. Genera **embeddings semÃ¡nticos** de cada oraciÃ³n
  2. Calcula **similitud coseno** entre oraciones consecutivas
  3. **Fallback**: AnÃ¡lisis de conectores lingÃ¼Ã­sticos si HF falla
  4. Considera cÃ³mo la respuesta mantiene coherencia con el prompt

#### **ðŸŽª Lic. Relevancia** (Peso: 20%)
- **Modelo HF**: `all-MiniLM-L6-v2` (SentenceTransformers)
- **TÃ©cnica**: Similitud semÃ¡ntica prompt â†” respuesta
- **EvalÃºa**: Pertinencia directa, completitud, foco temÃ¡tico
- **Proceso**:
  1. **Embeddings** separados de prompt y respuesta
  2. **Similitud semÃ¡ntica** entre ambos vectores
  3. **Fallback**: AnÃ¡lisis de overlap de palabras clave
  4. **Contexto**: Si el prompt hace una pregunta especÃ­fica, Â¿la respuesta la aborda?

#### **âš¡ Ed. Eficiencia** (Peso: 10%)
- **Herramienta**: `textstat` library + mÃ©tricas personalizadas
- **TÃ©cnica**: Ãndices de legibilidad + anÃ¡lisis de concisiÃ³n
- **EvalÃºa**: Claridad, longitud apropiada, facilidad de lectura
- **Algoritmos**:
  - **Ãndice Flesch** de legibilidad (>60 = fÃ¡cil de leer)
  - AnÃ¡lisis de longitud vs. complejidad del prompt
  - DetecciÃ³n si el prompt requiere respuesta detallada
  - MÃ©tricas de palabras por oraciÃ³n

#### **ðŸŽ¨ Dra. Creatividad** (Peso: 5%)
- **TÃ©cnica**: AnÃ¡lisis lÃ©xico-estadÃ­stico + heurÃ­sticas
- **EvalÃºa**: Originalidad, diversidad lÃ©xica, perspectivas Ãºnicas
- **Algoritmos**:
  - **Diversidad lÃ©xica**: unique_words / total_words
  - DetecciÃ³n de **indicadores creativos** (`"imaginemos", "supongamos", "metÃ¡fora"`)
  - AnÃ¡lisis de **variedad en estructura** de oraciones
  - Considera si el prompt solicita creatividad especÃ­ficamente

### **ðŸ”„ Robustez y Modos Fallback**

El sistema tiene **tolerancia a fallos** incorporada:

- **ðŸ”„ InicializaciÃ³n**: Timeout de 2 minutos, continua con fallbacks si HF falla
- **âš™ï¸ Fallback Inteligente**: Si `SentenceTransformers` no carga â†’ heurÃ­sticas de NLP
- **ðŸ›¡ï¸ Manejo de Errores**: Juez individual falla â†’ score neutro (5.0)
- **ðŸ“Š Consenso**: Analiza discrepancias entre jueces para detectar casos ambiguos

### **ðŸ“Š AnÃ¡lisis de Consenso**

```python
# Mide quÃ© tan de acuerdo estÃ¡n los 5 jueces
consensus_level = 1.0 - (std_dev_scores / 5.0)

# InterpretaciÃ³n:
# > 0.8 = Alto consenso (jueces de acuerdo)
# 0.6-0.8 = Consenso moderado (algunas diferencias)
# < 0.6 = Bajo consenso (evaluaciÃ³n compleja/ambigua)
```

### **ðŸŽ¯ EvaluaciÃ³n Contextual Prompt-Respuesta**

**CaracterÃ­stica Clave**: El sistema NO evalÃºa respuestas en el vacÃ­o.

âœ… **Lo que hace el sistema**:
- Analiza cÃ³mo la respuesta **responde especÃ­ficamente** al prompt
- Considera si el prompt requiere **creatividad, precisiÃ³n, o detalle**
- EvalÃºa la **relevancia semÃ¡ntica** entre pregunta y respuesta
- Ajusta expectativas segÃºn el **dominio** (tÃ©cnico, creativo, acadÃ©mico)

âŒ **Lo que NO hace**:
- Evaluar respuestas sin contexto del prompt original
- Aplicar criterios uniformes independientes de la pregunta
- Ignorar la intenciÃ³n y complejidad del prompt
