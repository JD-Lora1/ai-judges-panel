{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phi-2 AI Judge Evaluation Demo\n",
    "\n",
    "This notebook demonstrates how to use the Microsoft Phi-2 model for AI evaluation tasks.\n",
    "The Phi-2 model is an efficient 2.7B parameter transformer optimized for reasoning and code understanding.\n",
    "\n",
    "## Features:\n",
    "- Single model approach for comprehensive evaluation\n",
    "- Resource-efficient implementation\n",
    "- Support for multiple evaluation aspects\n",
    "- Batch processing capabilities\n",
    "- Response comparison functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required dependencies and import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../app')\n",
    "\n",
    "# Import our Phi-2 judge\n",
    "from models.phi2_judge import get_phi2_judge, Phi2Judge\n",
    "\n",
    "# Other imports\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ All dependencies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Phi-2 Judge\n",
    "\n",
    "Let's create an instance of our Phi-2 judge and check its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the singleton instance\n",
    "judge = get_phi2_judge()\n",
    "\n",
    "# Check model information\n",
    "model_info = judge.get_model_info()\n",
    "print(\"üìä Model Information:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Evaluation Example\n",
    "\n",
    "Let's start with a simple evaluation example to see how the Phi-2 judge works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sample prompt and response\n",
    "sample_prompt = \"Explain how machine learning works in simple terms.\"\n",
    "sample_response = \"\"\"\n",
    "Machine learning is like teaching a computer to recognize patterns and make decisions \n",
    "by showing it lots of examples. Instead of programming specific rules, we feed the \n",
    "computer data and let it learn from that data. For example, if we want to teach a \n",
    "computer to recognize cats in photos, we show it thousands of cat pictures. The \n",
    "computer finds patterns in these images and learns what makes a cat look like a cat. \n",
    "Then when we show it a new photo, it can predict whether there's a cat in it based \n",
    "on what it learned.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîç Evaluating sample prompt-response pair...\")\n",
    "print(f\"Prompt: {sample_prompt}\")\n",
    "print(f\"Response: {sample_response.strip()[:100]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the evaluation\n",
    "start_time = time.time()\n",
    "result = judge.evaluate(sample_prompt, sample_response)\n",
    "evaluation_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚è±Ô∏è Evaluation completed in {evaluation_time:.2f} seconds\")\n",
    "print(f\"üìä Overall Score: {result['overall_score']}/10\")\n",
    "print()\n",
    "print(\"üìã Detailed Scores:\")\n",
    "for aspect, score in result['detailed_scores'].items():\n",
    "    print(f\"  {aspect.capitalize()}: {score}/10\")\n",
    "\n",
    "print()\n",
    "print(\"üí¨ Detailed Feedback:\")\n",
    "for aspect, feedback in result['detailed_feedback'].items():\n",
    "    print(f\"  {aspect.capitalize()}: {feedback[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Weight Evaluation\n",
    "\n",
    "You can customize the importance of different evaluation aspects by providing custom weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom weights (prioritizing relevance and accuracy)\n",
    "custom_weights = {\n",
    "    \"relevance\": 0.4,\n",
    "    \"accuracy\": 0.4,\n",
    "    \"coherence\": 0.15,\n",
    "    \"completeness\": 0.05\n",
    "}\n",
    "\n",
    "print(\"üéØ Custom weighted evaluation:\")\n",
    "print(f\"Weights: {custom_weights}\")\n",
    "print()\n",
    "\n",
    "# Evaluate with custom weights\n",
    "weighted_result = judge.evaluate(sample_prompt, sample_response, weights=custom_weights)\n",
    "\n",
    "print(f\"üìä Weighted Overall Score: {weighted_result['overall_score']}/10\")\n",
    "print(f\"üìä Default Overall Score: {result['overall_score']}/10\")\n",
    "print(f\"üìà Difference: {abs(weighted_result['overall_score'] - result['overall_score']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Comparison\n",
    "\n",
    "Let's compare two different responses to the same prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two responses to compare\n",
    "comparison_prompt = \"What are the benefits of renewable energy?\"\n",
    "\n",
    "response_a = \"\"\"\n",
    "Renewable energy has several key benefits: it's environmentally friendly because it doesn't \n",
    "produce greenhouse gas emissions, it's sustainable since sources like solar and wind are \n",
    "naturally replenished, and it can reduce long-term energy costs once the infrastructure \n",
    "is established. Additionally, it helps reduce dependence on fossil fuel imports and creates \n",
    "jobs in the green energy sector.\n",
    "\"\"\"\n",
    "\n",
    "response_b = \"\"\"\n",
    "Renewable energy is good for the planet. Solar panels and wind turbines make electricity \n",
    "without pollution. It's better than coal and oil.\n",
    "\"\"\"\n",
    "\n",
    "print(\"üîÑ Comparing two responses:\")\n",
    "print(f\"Prompt: {comparison_prompt}\")\n",
    "print(f\"Response A: {response_a.strip()[:80]}...\")\n",
    "print(f\"Response B: {response_b.strip()[:80]}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comparison\n",
    "comparison_result = judge.compare_responses(comparison_prompt, response_a, response_b)\n",
    "\n",
    "print(\"üèÜ Comparison Results:\")\n",
    "print(f\"Winner: {comparison_result['winner']}\")\n",
    "print(f\"Margin: {comparison_result['margin']} points\")\n",
    "print()\n",
    "print(\"üìä Score Summary:\")\n",
    "summary = comparison_result['comparison_summary']\n",
    "print(f\"Response A Score: {summary['response1_score']}/10\")\n",
    "print(f\"Response B Score: {summary['response2_score']}/10\")\n",
    "print(f\"Difference: {summary['difference']} points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Evaluation\n",
    "\n",
    "For evaluating multiple prompt-response pairs efficiently, we can use batch evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of evaluations\n",
    "batch_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain quantum computing\",\n",
    "        \"response\": \"Quantum computing uses quantum mechanical phenomena like superposition and entanglement to perform computations that would be impossible for classical computers.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What is artificial intelligence?\",\n",
    "        \"response\": \"AI is the simulation of human intelligence in machines that are programmed to think and learn like humans.\"\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How do neural networks work?\",\n",
    "        \"response\": \"Neural networks are computing systems inspired by biological neural networks. They consist of interconnected nodes that process information.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"üîÑ Processing batch of {len(batch_data)} evaluations...\")\n",
    "batch_results = judge.batch_evaluate(batch_data)\n",
    "\n",
    "print(\"‚úÖ Batch evaluation completed!\")\n",
    "print()\n",
    "print(\"üìä Batch Results Summary:\")\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"  Evaluation {i+1}: {result['overall_score']}/10 (Time: {result['evaluation_time']}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's create some visualizations to better understand the evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for visualization\n",
    "aspects = list(result['detailed_scores'].keys())\n",
    "scores = list(result['detailed_scores'].values())\n",
    "weights = list(result['weights_used'].values())\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Score breakdown\n",
    "bars1 = ax1.bar(aspects, scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "ax1.set_title('Evaluation Scores by Aspect', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Score (1-10)')\n",
    "ax1.set_ylim(0, 10)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add score labels on bars\n",
    "for bar, score in zip(bars1, scores):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Weights visualization\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "wedges, texts, autotexts = ax2.pie(weights, labels=aspects, colors=colors, \n",
    "                                   autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Aspect Weights Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Make percentage text bold\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontweight('bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of our Phi-2 judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect performance data from batch results\n",
    "performance_data = {\n",
    "    'evaluation_times': [result['evaluation_time'] for result in batch_results],\n",
    "    'prompt_lengths': [result['input_info']['prompt_length'] for result in batch_results],\n",
    "    'response_lengths': [result['input_info']['response_length'] for result in batch_results],\n",
    "    'overall_scores': [result['overall_score'] for result in batch_results]\n",
    "}\n",
    "\n",
    "# Create DataFrame for analysis\n",
    "df = pd.DataFrame(performance_data)\n",
    "\n",
    "print(\"üìä Performance Statistics:\")\n",
    "print(f\"Average evaluation time: {df['evaluation_times'].mean():.2f}s\")\n",
    "print(f\"Min evaluation time: {df['evaluation_times'].min():.2f}s\")\n",
    "print(f\"Max evaluation time: {df['evaluation_times'].max():.2f}s\")\n",
    "print(f\"Average overall score: {df['overall_scores'].mean():.2f}/10\")\n",
    "print(f\"Score standard deviation: {df['overall_scores'].std():.2f}\")\n",
    "\n",
    "print(\"\\nüìã Performance Summary Table:\")\n",
    "print(df.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Edge Cases\n",
    "\n",
    "Let's test some edge cases to see how robust our evaluation system is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "edge_cases = [\n",
    "    {\n",
    "        \"name\": \"Irrelevant Response\",\n",
    "        \"prompt\": \"Explain photosynthesis\",\n",
    "        \"response\": \"I love pizza. It's my favorite food. The weather today is nice.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Empty Response\",\n",
    "        \"prompt\": \"What is the capital of France?\",\n",
    "        \"response\": \"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Perfect Match\",\n",
    "        \"prompt\": \"What is 2+2?\",\n",
    "        \"response\": \"2+2 equals 4. This is basic arithmetic.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Very Short Response\",\n",
    "        \"prompt\": \"Explain the theory of relativity\",\n",
    "        \"response\": \"Einstein.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Edge Cases:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "edge_results = []\n",
    "for case in edge_cases:\n",
    "    print(f\"\\nüîç {case['name']}:\")\n",
    "    print(f\"Prompt: {case['prompt']}\")\n",
    "    print(f\"Response: '{case['response']}'\")\n",
    "    \n",
    "    result = judge.evaluate(case['prompt'], case['response'])\n",
    "    edge_results.append(result)\n",
    "    \n",
    "    print(f\"Overall Score: {result['overall_score']}/10\")\n",
    "    print(f\"Relevance: {result['detailed_scores']['relevance']}/10\")\n",
    "    print(f\"Evaluation Time: {result['evaluation_time']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Memory Management\n",
    "\n",
    "The Phi-2 judge includes memory management features. Let's demonstrate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current model status\n",
    "model_info = judge.get_model_info()\n",
    "print(f\"Model loaded: {model_info['model_loaded']}\")\n",
    "print(f\"Device: {model_info['device']}\")\n",
    "\n",
    "# Demonstrate model unloading (optional for memory management)\n",
    "print(\"\\nüîÑ Unloading model to free memory...\")\n",
    "judge.unload_model()\n",
    "\n",
    "# Check status after unloading\n",
    "model_info = judge.get_model_info()\n",
    "print(f\"Model loaded after unload: {model_info['model_loaded']}\")\n",
    "\n",
    "# Model will be reloaded automatically on next evaluation\n",
    "print(\"\\nüîÑ Model will be reloaded automatically on next evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the key features of our Phi-2 AI Judge:\n",
    "\n",
    "‚úÖ **Single Model Architecture**: Uses only Microsoft Phi-2 for all evaluations\n",
    "\n",
    "‚úÖ **Resource Efficient**: Optimized for memory usage and performance\n",
    "\n",
    "‚úÖ **Comprehensive Evaluation**: Covers relevance, coherence, accuracy, and completeness\n",
    "\n",
    "‚úÖ **Flexible Weighting**: Allows custom importance weights for different aspects\n",
    "\n",
    "‚úÖ **Batch Processing**: Efficient handling of multiple evaluations\n",
    "\n",
    "‚úÖ **Response Comparison**: Side-by-side comparison of different responses\n",
    "\n",
    "‚úÖ **Memory Management**: Built-in model loading/unloading for resource management\n",
    "\n",
    "‚úÖ **Edge Case Handling**: Robust evaluation even with problematic inputs\n",
    "\n",
    "### Next Steps:\n",
    "- Integrate this judge into your web application\n",
    "- Fine-tune evaluation prompts for your specific use case\n",
    "- Experiment with different aspect weights\n",
    "- Monitor performance in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéâ Phi-2 AI Judge Demo Complete!\")\n",
    "print(\"Ready for production deployment.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
