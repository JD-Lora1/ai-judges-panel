{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro"
      },
      "source": [
        "# üèõÔ∏è AI Judges Panel - Demostraci√≥n B√°sica\n",
        "## Evaluaci√≥n Multi-Agente de LLMs\n",
        "\n",
        "Bienvenido al **AI Judges Panel**, donde m√∫ltiples LLMs especializados eval√∫an respuestas de otros LLMs desde diferentes perspectivas.\n",
        "\n",
        "### üéØ Lo que aprender√°s:\n",
        "- C√≥mo funciona la arquitectura de panel de jueces\n",
        "- Evaluaci√≥n multi-dimensional autom√°tica\n",
        "- Interpretaci√≥n de consensos y discrepancias\n",
        "- Comparaci√≥n con m√©tricas autom√°ticas tradicionales\n",
        "\n",
        "### üß† Panel de Jueces:\n",
        "- **üéØ Dr. Precisi√≥n**: Eval√∫a factualidad y exactitud\n",
        "- **üé® Dra. Creatividad**: Eval√∫a originalidad e innovaci√≥n  \n",
        "- **üß† Prof. Coherencia**: Eval√∫a l√≥gica y estructura\n",
        "- **üé™ Lic. Relevancia**: Eval√∫a pertinencia a la pregunta\n",
        "- **‚ö° Ed. Eficiencia**: Eval√∫a claridad y concisi√≥n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. üîß Setup e Importaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "# Importaciones necesarias\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Agregar el path del proyecto\n",
        "project_path = os.path.abspath('../src')\n",
        "if project_path not in sys.path:\n",
        "    sys.path.append(project_path)\n",
        "\n",
        "# Importar nuestras clases\n",
        "from evaluators.meta_evaluator import MetaEvaluator, ComprehensiveEvaluation\n",
        "from judges.precision_judge import PrecisionJudge\n",
        "from judges.base_judge import EvaluationContext\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML, Markdown\n",
        "\n",
        "print(\"‚úÖ Importaciones completadas\")\n",
        "print(\"üèõÔ∏è AI Judges Panel listo para usar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "initialize"
      },
      "source": [
        "## 2. üèóÔ∏è Inicializaci√≥n del Panel de Jueces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "init_panel"
      },
      "outputs": [],
      "source": [
        "# Inicializar el Meta-Evaluador con panel completo\n",
        "evaluator = MetaEvaluator()\n",
        "\n",
        "print(\"üèõÔ∏è Panel de Jueces Inicializado:\")\n",
        "print(f\"üìä Total de jueces: {len(evaluator.judges)}\")\n",
        "print(\"\\nüë®‚Äç‚öñÔ∏è Jueces especializados:\")\n",
        "for judge in evaluator.judges:\n",
        "    print(f\"  - {judge}\")\n",
        "    \n",
        "print(\"\\n‚öñÔ∏è Pesos de evaluaci√≥n:\")\n",
        "for aspect, weight in evaluator.weights.items():\n",
        "    print(f\"  - {aspect.title()}: {weight:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_data"
      },
      "source": [
        "## 3. üìù Casos de Prueba\n",
        "\n",
        "Vamos a evaluar diferentes respuestas a la misma pregunta para ver c√≥mo reacciona nuestro panel de jueces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_cases"
      },
      "outputs": [],
      "source": [
        "# Definir casos de prueba\n",
        "test_cases = {\n",
        "    \"prompt\": \"Explica qu√© es la inteligencia artificial y c√≥mo impacta la sociedad moderna.\",\n",
        "    \n",
        "    \"responses\": {\n",
        "        \"Respuesta A - T√©cnica\": \"\"\"\n",
        "La inteligencia artificial (IA) es un campo de la ciencia de la computaci√≥n que se enfoca en crear sistemas \n",
        "capaces de realizar tareas que normalmente requieren inteligencia humana. Estas tareas incluyen el aprendizaje, \n",
        "el reconocimiento de patrones, la toma de decisiones y el procesamiento del lenguaje natural.\n",
        "\n",
        "La IA utiliza algoritmos de machine learning, redes neuronales y deep learning para procesar grandes cantidades \n",
        "de datos y extraer patrones significativos. Los sistemas de IA pueden clasificarse en IA d√©bil (espec√≠fica para \n",
        "tareas particulares) e IA fuerte (inteligencia general).\n",
        "\n",
        "En la sociedad moderna, la IA impacta m√∫ltiples sectores: automatiza procesos en la industria, mejora diagn√≥sticos \n",
        "m√©dicos, optimiza sistemas de transporte, personaliza experiencias digitales y facilita la investigaci√≥n cient√≠fica. \n",
        "Sin embargo, tambi√©n plantea desaf√≠os √©ticos sobre privacidad, sesgo algor√≠tmico y desplazamiento laboral.\n",
        "\"\"\",\n",
        "        \n",
        "        \"Respuesta B - Creativa\": \"\"\"\n",
        "Imagina que la humanidad ha creado un nuevo tipo de \"cerebro digital\" - eso es la inteligencia artificial. \n",
        "Como un aprendiz incansable, la IA nunca se cansa de absorber informaci√≥n y encontrar patrones ocultos \n",
        "que nosotros podr√≠amos pasar por alto.\n",
        "\n",
        "La IA es como tener millones de asistentes especializados trabajando 24/7: uno que nunca se equivoca \n",
        "en matem√°ticas, otro que puede 'ver' tumores en radiograf√≠as mejor que m√©dicos experimentados, y otro \n",
        "que predice qu√© pel√≠cula te gustar√° antes de que t√∫ lo sepas.\n",
        "\n",
        "Pero aqu√≠ est√° el giro fascinante: estamos creando una sociedad simbi√≥tica. La IA no est√° reemplazando \n",
        "la inteligencia humana; est√° amplific√°ndola. Somos como los directores de una orquesta tecnol√≥gica, \n",
        "dirigiendo sinfon√≠as de datos hacia soluciones que beneficien a la humanidad.\n",
        "\n",
        "El verdadero impacto no es solo automatizaci√≥n - es liberaci√≥n. Liberaci√≥n del trabajo repetitivo \n",
        "para enfocarnos en creatividad, innovaci√≥n y conexi√≥n humana genuina.\n",
        "\"\"\",\n",
        "        \n",
        "        \"Respuesta C - Superficial\": \"\"\"\n",
        "La IA es cuando las computadoras son inteligentes como los humanos. Se usa en muchas cosas \n",
        "como Google y Netflix. Es buena porque ayuda a las personas pero tambi√©n puede ser mala \n",
        "porque puede quitar trabajos. La IA est√° en todas partes y va a cambiar el mundo.\n",
        "\"\"\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"üìù Casos de prueba definidos:\")\n",
        "print(f\"‚ùì Pregunta: {test_cases['prompt']}\")\n",
        "print(f\"üìä Respuestas a evaluar: {len(test_cases['responses'])}\")\n",
        "for name in test_cases['responses'].keys():\n",
        "    print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaluation_demo"
      },
      "source": [
        "## 4. üèõÔ∏è Evaluaci√≥n del Panel de Jueces\n",
        "\n",
        "Ahora vamos a someter cada respuesta al panel completo de jueces especializados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_evaluations"
      },
      "outputs": [],
      "source": [
        "# Ejecutar evaluaciones\n",
        "results = {}\n",
        "\n",
        "print(\"üèõÔ∏è Iniciando evaluaciones del panel...\\n\")\n",
        "\n",
        "for response_name, response_text in test_cases[\"responses\"].items():\n",
        "    print(f\"üìù Evaluando: {response_name}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Realizar evaluaci√≥n comprehensiva\n",
        "    evaluation = evaluator.evaluate(\n",
        "        prompt=test_cases[\"prompt\"],\n",
        "        response=response_text,\n",
        "        domain=\"AI Education\",\n",
        "        include_automatic_metrics=True\n",
        "    )\n",
        "    \n",
        "    results[response_name] = evaluation\n",
        "    \n",
        "    # Mostrar resultados inmediatos\n",
        "    print(f\"üèÜ Score Final: {evaluation.final_score:.1f}/10\")\n",
        "    print(f\"ü§ù Consenso: {evaluation.consensus_level:.1%}\")\n",
        "    print(f\"‚è±Ô∏è Tiempo: {evaluation.evaluation_time:.2f}s\")\n",
        "    \n",
        "    print(\"\\nüìä Scores por aspecto:\")\n",
        "    for aspect, score in evaluation.individual_scores.items():\n",
        "        print(f\"  {aspect.title()}: {score:.1f}/10\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Top 3 Fortalezas:\")\n",
        "    for i, strength in enumerate(evaluation.strengths[:3], 1):\n",
        "        print(f\"  {i}. {strength}\")\n",
        "        \n",
        "    print(\"\\nüîß Top 3 Mejoras:\")\n",
        "    for i, improvement in enumerate(evaluation.improvements[:3], 1):\n",
        "        print(f\"  {i}. {improvement}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Todas las evaluaciones completadas!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison"
      },
      "source": [
        "## 5. üìä An√°lisis Comparativo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_comparison"
      },
      "outputs": [],
      "source": [
        "# Crear DataFrame para an√°lisis\n",
        "comparison_data = []\n",
        "\n",
        "for response_name, evaluation in results.items():\n",
        "    row = {\n",
        "        'Response': response_name,\n",
        "        'Final Score': evaluation.final_score,\n",
        "        'Consensus': evaluation.consensus_level,\n",
        "        'Time (s)': evaluation.evaluation_time\n",
        "    }\n",
        "    \n",
        "    # Agregar scores individuales\n",
        "    for aspect, score in evaluation.individual_scores.items():\n",
        "        row[aspect.title()] = score\n",
        "        \n",
        "    comparison_data.append(row)\n",
        "\n",
        "df = pd.DataFrame(comparison_data)\n",
        "display(df.round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualizations"
      },
      "outputs": [],
      "source": [
        "# Visualizaciones comparativas\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Scores finales\n",
        "axes[0,0].bar(df['Response'], df['Final Score'], color=['#1f77b4', '#ff7f0e', '#2ca02c'])\n",
        "axes[0,0].set_title('üèÜ Score Final por Respuesta', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_ylabel('Score (0-10)')\n",
        "axes[0,0].set_ylim(0, 10)\n",
        "for i, v in enumerate(df['Final Score']):\n",
        "    axes[0,0].text(i, v + 0.1, f'{v:.1f}', ha='center', fontweight='bold')\n",
        "plt.setp(axes[0,0].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# 2. Nivel de consenso\n",
        "axes[0,1].bar(df['Response'], df['Consensus'], color=['#d62728', '#9467bd', '#8c564b'])\n",
        "axes[0,1].set_title('ü§ù Nivel de Consenso entre Jueces', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_ylabel('Consenso (0-1)')\n",
        "axes[0,1].set_ylim(0, 1)\n",
        "for i, v in enumerate(df['Consensus']):\n",
        "    axes[0,1].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
        "plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# 3. Heatmap de scores por aspecto\n",
        "aspect_cols = ['Precision', 'Creativity', 'Coherence', 'Relevance', 'Efficiency']\n",
        "heatmap_data = df[aspect_cols].T  # Transponer para mejor visualizaci√≥n\n",
        "heatmap_data.columns = df['Response']\n",
        "\n",
        "sns.heatmap(heatmap_data, annot=True, cmap='RdYlGn', center=5, \n",
        "            vmin=0, vmax=10, ax=axes[1,0], cbar_kws={'label': 'Score (0-10)'})\n",
        "axes[1,0].set_title('üéØ Scores por Aspecto y Respuesta', fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_ylabel('Aspecto Evaluado')\n",
        "\n",
        "# 4. Radar chart para la mejor respuesta\n",
        "best_response_idx = df['Final Score'].idxmax()\n",
        "best_response_name = df.loc[best_response_idx, 'Response']\n",
        "best_scores = df.loc[best_response_idx, aspect_cols].values\n",
        "\n",
        "angles = np.linspace(0, 2*np.pi, len(aspect_cols), endpoint=False)\n",
        "angles = np.concatenate((angles, [angles[0]]))  # Cerrar el c√≠rculo\n",
        "best_scores = np.concatenate((best_scores, [best_scores[0]]))\n",
        "\n",
        "axes[1,1] = plt.subplot(2, 2, 4, projection='polar')\n",
        "axes[1,1].plot(angles, best_scores, 'o-', linewidth=2, color='#ff7f0e')\n",
        "axes[1,1].fill(angles, best_scores, alpha=0.25, color='#ff7f0e')\n",
        "axes[1,1].set_xticks(angles[:-1])\n",
        "axes[1,1].set_xticklabels(aspect_cols)\n",
        "axes[1,1].set_ylim(0, 10)\n",
        "axes[1,1].set_title(f'üåü Perfil de {best_response_name}', fontsize=14, fontweight='bold', pad=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nüéâ An√°lisis visual completado!\")\n",
        "print(f\"üèÜ Mejor respuesta: {best_response_name} (Score: {df.loc[best_response_idx, 'Final Score']:.1f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed_analysis"
      },
      "source": [
        "## 6. üîç An√°lisis Detallado del Juez de Precisi√≥n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "precision_analysis"
      },
      "outputs": [],
      "source": [
        "# Obtener el juez de precisi√≥n espec√≠ficamente\n",
        "precision_judge = None\n",
        "for judge in evaluator.judges:\n",
        "    if judge.name == \"Dr. Precisi√≥n\":\n",
        "        precision_judge = judge\n",
        "        break\n",
        "\n",
        "if precision_judge:\n",
        "    print(\"üéØ An√°lisis Detallado del Dr. Precisi√≥n\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Obtener reporte de precisi√≥n\n",
        "    precision_report = precision_judge.get_precision_report()\n",
        "    \n",
        "    print(f\"üìä Total de evaluaciones realizadas: {precision_report['total_evaluations']}\")\n",
        "    print(f\"‚úÖ Porcentaje de alta precisi√≥n (‚â•8.0): {precision_report['high_precision_percentage']:.1f}%\")\n",
        "    print(f\"‚ùå Porcentaje de baja precisi√≥n (<5.0): {precision_report['low_precision_percentage']:.1f}%\")\n",
        "    print(f\"üéØ Confianza promedio del juez: {precision_report['average_confidence']:.1%}\")\n",
        "    print(f\"üîí Confiabilidad del juez: {precision_report['judge_reliability']}\")\n",
        "    \n",
        "    # An√°lisis detallado de cada evaluaci√≥n de precisi√≥n\n",
        "    print(\"\\nüîç Detalles de Evaluaciones de Precisi√≥n:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for response_name, evaluation in results.items():\n",
        "        if 'precision' in evaluation.detailed_feedback:\n",
        "            precision_eval = evaluation.detailed_feedback['precision']\n",
        "            print(f\"\\nüìù {response_name}:\")\n",
        "            print(f\"   Score: {precision_eval.score:.1f}/10\")\n",
        "            print(f\"   Confianza: {precision_eval.confidence:.1%}\")\n",
        "            print(f\"   Razonamiento: {precision_eval.reasoning[:200]}...\")\n",
        "            \n",
        "            # Informaci√≥n espec√≠fica de precisi√≥n (si est√° disponible)\n",
        "            if hasattr(precision_eval, 'metadata') and precision_eval.metadata:\n",
        "                if 'factual_claims' in precision_eval.metadata:\n",
        "                    claims = precision_eval.metadata.get('factual_claims', [])\n",
        "                    if claims:\n",
        "                        print(f\"   Afirmaciones factuales detectadas: {len(claims)}\")\n",
        "                        \n",
        "                if 'potential_hallucinations' in precision_eval.metadata:\n",
        "                    hallucinations = precision_eval.metadata.get('potential_hallucinations', [])\n",
        "                    if hallucinations:\n",
        "                        print(f\"   ‚ö†Ô∏è Posibles alucinaciones: {len(hallucinations)}\")\n",
        "                    else:\n",
        "                        print(f\"   ‚úÖ No se detectaron alucinaciones\")\n",
        "else:\n",
        "    print(\"‚ùå No se encontr√≥ el Juez de Precisi√≥n en el panel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "panel_summary"
      },
      "source": [
        "## 7. üìà Resumen del Panel Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "get_panel_summary"
      },
      "outputs": [],
      "source": [
        "# Obtener resumen completo del panel\n",
        "panel_summary = evaluator.get_panel_summary()\n",
        "\n",
        "print(\"üèõÔ∏è RESUMEN DEL PANEL DE JUECES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"üìä Evaluaciones totales realizadas: {panel_summary['total_evaluations']}\")\n",
        "print(f\"üèÜ Score promedio final: {panel_summary['average_final_score']}/10\")\n",
        "print(f\"ü§ù Consenso promedio: {panel_summary['average_consensus']:.1%}\")\n",
        "print(f\"‚è±Ô∏è Tiempo promedio de evaluaci√≥n: {panel_summary['average_evaluation_time']:.2f}s\")\n",
        "print(f\"üë®‚Äç‚öñÔ∏è Total de jueces activos: {panel_summary['judges_count']}\")\n",
        "\n",
        "print(\"\\nüéØ RENDIMIENTO POR ASPECTO:\")\n",
        "print(\"-\" * 40)\n",
        "for aspect, avg_score in panel_summary['aspect_performance'].items():\n",
        "    weight = panel_summary['weights_distribution'].get(aspect, 0)\n",
        "    print(f\"{aspect.title():12} | Score: {avg_score:4.1f} | Peso: {weight:4.1%}\")\n",
        "\n",
        "# Crear visualizaci√≥n del rendimiento del panel\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Gr√°fico de rendimiento por aspecto\n",
        "aspects = list(panel_summary['aspect_performance'].keys())\n",
        "scores = list(panel_summary['aspect_performance'].values())\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(aspects)))\n",
        "\n",
        "bars = ax1.bar(aspects, scores, color=colors)\n",
        "ax1.set_title('üìä Rendimiento Promedio por Aspecto', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylabel('Score Promedio (0-10)')\n",
        "ax1.set_ylim(0, 10)\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for bar, score in zip(bars, scores):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Gr√°fico de distribuci√≥n de pesos\n",
        "weights = list(panel_summary['weights_distribution'].values())\n",
        "ax2.pie(weights, labels=aspects, autopct='%1.1f%%', startangle=90, colors=colors)\n",
        "ax2.set_title('‚öñÔ∏è Distribuci√≥n de Pesos en Evaluaci√≥n', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚ú® Panel summary visualizado exitosamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_evaluation"
      },
      "source": [
        "## 8. üîß Evaluaci√≥n Personalizada\n",
        "\n",
        "¬°Ahora es tu turno! Ingresa tu propia pregunta y respuesta para ver c√≥mo las eval√∫a nuestro panel de jueces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "custom_input"
      },
      "outputs": [],
      "source": [
        "# Input personalizable\n",
        "custom_prompt = \"\"\"¬øCu√°les son los principales desaf√≠os √©ticos de la inteligencia artificial \n",
        "y c√≥mo deber√≠an abordarse desde una perspectiva de pol√≠tica p√∫blica?\"\"\"\n",
        "\n",
        "custom_response = \"\"\"Los principales desaf√≠os √©ticos de la IA incluyen:\n",
        "\n",
        "1. **Sesgo algor√≠tmico**: Los sistemas de IA pueden perpetuar o amplificar sesgos existentes en los datos de entrenamiento.\n",
        "\n",
        "2. **Privacidad y vigilancia**: La capacidad de la IA para procesar grandes cantidades de datos personales plantea preocupaciones sobre la privacidad.\n",
        "\n",
        "3. **Transparencia y explicabilidad**: Los modelos de \"caja negra\" dificultan entender c√≥mo se toman las decisiones.\n",
        "\n",
        "4. **Responsabilidad**: ¬øQui√©n es responsable cuando un sistema de IA causa da√±o?\n",
        "\n",
        "Desde pol√≠tica p√∫blica, se debe:\n",
        "- Establecer marcos regulatorios claros pero flexibles\n",
        "- Promover auditor√≠as algor√≠tmicas obligatorias\n",
        "- Invertir en educaci√≥n y capacitaci√≥n √©tica para desarrolladores\n",
        "- Fomentar la participaci√≥n ciudadana en el desarrollo de pol√≠ticas de IA\"\"\"\n",
        "\n",
        "print(\"üîß Evaluaci√≥n Personalizada\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"‚ùì Pregunta: {custom_prompt}\")\n",
        "print(f\"\\nüìù Respuesta a evaluar:\")\n",
        "print(custom_response)\n",
        "print(\"\\n\" + \"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_custom_evaluation"
      },
      "outputs": [],
      "source": [
        "# Ejecutar evaluaci√≥n personalizada\n",
        "print(\"üèõÔ∏è Evaluando respuesta personalizada...\\n\")\n",
        "\n",
        "custom_evaluation = evaluator.evaluate(\n",
        "    prompt=custom_prompt,\n",
        "    response=custom_response,\n",
        "    domain=\"AI Ethics & Policy\",\n",
        "    include_automatic_metrics=False  # Sin respuesta de referencia\n",
        ")\n",
        "\n",
        "print(\"üéâ ¬°RESULTADOS DE TU EVALUACI√ìN PERSONALIZADA!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"üèÜ SCORE FINAL: {custom_evaluation.final_score:.1f}/10\")\n",
        "print(f\"ü§ù Consenso entre jueces: {custom_evaluation.consensus_level:.1%}\")\n",
        "print(f\"‚è±Ô∏è Tiempo de evaluaci√≥n: {custom_evaluation.evaluation_time:.2f} segundos\")\n",
        "\n",
        "print(\"\\nüìä SCORES DETALLADOS POR JUEZ:\")\n",
        "print(\"-\" * 40)\n",
        "for aspect, score in custom_evaluation.individual_scores.items():\n",
        "    emoji = {\"precision\": \"üéØ\", \"creativity\": \"üé®\", \"coherence\": \"üß†\", \n",
        "             \"relevance\": \"üé™\", \"efficiency\": \"‚ö°\"}.get(aspect, \"üìä\")\n",
        "    print(f\"{emoji} {aspect.title():12} | {score:.1f}/10\")\n",
        "\n",
        "print(\"\\n‚úÖ PRINCIPALES FORTALEZAS:\")\n",
        "print(\"-\" * 30)\n",
        "for i, strength in enumerate(custom_evaluation.strengths[:5], 1):\n",
        "    print(f\"{i}. {strength}\")\n",
        "\n",
        "print(\"\\nüîß √ÅREAS DE MEJORA:\")\n",
        "print(\"-\" * 20)\n",
        "for i, improvement in enumerate(custom_evaluation.improvements[:5], 1):\n",
        "    print(f\"{i}. {improvement}\")\n",
        "\n",
        "print(\"\\nüéØ AN√ÅLISIS DE CONSENSO:\")\n",
        "if custom_evaluation.consensus_level > 0.8:\n",
        "    print(\"‚úÖ Alto consenso: Los jueces est√°n muy de acuerdo en su evaluaci√≥n\")\n",
        "elif custom_evaluation.consensus_level > 0.6:\n",
        "    print(\"‚öñÔ∏è Consenso moderado: Hay algunas diferencias de opini√≥n entre jueces\")\n",
        "else:\n",
        "    print(\"ü§î Bajo consenso: Los jueces tienen perspectivas muy diferentes\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéä ¬°Evaluaci√≥n personalizada completada!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusions"
      },
      "source": [
        "## 9. üéØ Conclusiones y Pr√≥ximos Pasos\n",
        "\n",
        "### üîç ¬øQu√© hemos aprendido?\n",
        "\n",
        "1. **Evaluaci√≥n Multi-Dimensional**: Cada aspecto (precisi√≥n, creatividad, coherencia, etc.) proporciona insights √∫nicos\n",
        "2. **Consenso vs. Discrepancia**: Cuando los jueces no est√°n de acuerdo, indica complejidad en la evaluaci√≥n\n",
        "3. **Especializaci√≥n Importa**: Cada juez aporta expertise espec√≠fico que enriquece la evaluaci√≥n final\n",
        "4. **Interpretabilidad**: A diferencia de m√©tricas autom√°ticas, obtenemos explicaciones detalladas\n",
        "\n",
        "### üöÄ Ventajas del AI Judges Panel:\n",
        "- ‚úÖ **Evaluaci√≥n hol√≠stica** desde m√∫ltiples perspectivas\n",
        "- ‚úÖ **Feedback explicable** y accionable\n",
        "- ‚úÖ **Detecci√≥n de consensos** y discrepancias\n",
        "- ‚úÖ **Adaptable** a diferentes dominios y contextos\n",
        "- ‚úÖ **Escalable** - f√°cil agregar nuevos jueces especializados\n",
        "\n",
        "### üîß Pr√≥ximos Pasos:\n",
        "1. **Experimenta** con diferentes tipos de prompts y respuestas\n",
        "2. **Personaliza pesos** seg√∫n tu caso de uso espec√≠fico\n",
        "3. **Agrega jueces especializados** para tu dominio\n",
        "4. **Integra m√©tricas autom√°ticas** reales (BLEU, ROUGE, BERTScore)\n",
        "5. **Conecta con APIs reales** de LLMs para evaluaci√≥n en vivo\n",
        "\n",
        "### üìö Para Aprender M√°s:\n",
        "- Revisa `02_judge_comparison.ipynb` para an√°lisis detallado de cada juez\n",
        "- Explora `03_model_benchmarking.ipynb` para comparar diferentes LLMs\n",
        "- Consulta `04_custom_evaluation.ipynb` para casos de uso especializados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "final_summary"
      },
      "outputs": [],
      "source": [
        "# Resumen final de la sesi√≥n\n",
        "total_evaluations = len(results) + 1  # +1 por la evaluaci√≥n personalizada\n",
        "avg_final_score = np.mean([eval.final_score for eval in results.values()] + [custom_evaluation.final_score])\n",
        "avg_consensus = np.mean([eval.consensus_level for eval in results.values()] + [custom_evaluation.consensus_level])\n",
        "\n",
        "print(\"üéä RESUMEN FINAL DE LA SESI√ìN\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"üìä Total de evaluaciones realizadas: {total_evaluations}\")\n",
        "print(f\"üèÜ Score promedio obtenido: {avg_final_score:.1f}/10\")\n",
        "print(f\"ü§ù Consenso promedio del panel: {avg_consensus:.1%}\")\n",
        "print(f\"üèõÔ∏è Jueces especializados utilizados: {len(evaluator.judges)}\")\n",
        "\n",
        "print(\"\\nüåü ¬°Gracias por explorar el AI Judges Panel!\")\n",
        "print(\"La evaluaci√≥n multi-agente representa el futuro de la\")\n",
        "print(\"evaluaci√≥n de IA: m√°s robusta, interpretable y confiable.\")\n",
        "\n",
        "print(\"\\nüöÄ ¬°Contin√∫a experimentando y construyendo el futuro de la evaluaci√≥n de IA!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
